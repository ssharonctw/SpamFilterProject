{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef99151-fd0f-4296-b24b-84897fc25254",
   "metadata": {},
   "source": [
    "# Spam Message Classifier \n",
    "### built with NLTK and Scikit-learn (acheiving 99% accuracy)\n",
    "\n",
    "## Data Source Overview\n",
    "The data set is from the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). It contains 5000 SMS labeled messages that have been collected for mobile phone spam research. \n",
    "\n",
    "## Project Overview\n",
    "<details>\n",
    "    <summary>1. Load the Dataset</summary>\n",
    "    <p>Use the pandas dataframe to ensure value are properly loaded</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>2. Data Preprocessing</summary>\n",
    "    <p>Convert the class labels (span/ham) to binray values uesing sklearn LabelEncoder. Replace email address, url, phonenumbers with generic representation. Then remove stop words and keepwing word stems</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>3. Features Generating</summary>\n",
    "    <p>View words as features, choosing the top 500-1500 common words as features. Then process each sentence and generate the word_feature table for each sentence</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>4. Model Training and Performance Evaluation</summary>\n",
    "    <p>Split data set to 75% training data and 25% testing data. Train models with different algorithms (including ensemble methods of classifier voting) and compare the performances.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b564b6-7ac4-4b92-a867-9a1aefb0ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prestart: Importing major libraries import\n",
    "import sys #for printing ...etc\n",
    "import nltk #for handling NLP techniques\n",
    "import sklearn #for training ML models\n",
    "import pandas #for sotring data in dataframes\n",
    "import numpy #for basic computational tasks\n",
    "\n",
    "#checking libraries are correctly installed\n",
    "#print('Python: {}'.format(sys.version))\n",
    "#print('NLTK: {}'.format(nltk.__version__))\n",
    "#print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "#print('Pandas: {}'.format(pandas.__version__))\n",
    "#print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661b0e8-353d-4d3e-8505-f91092a6d737",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7ba3c8-a257-4566-9633-898d60c9283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load the dataset of sms messages\n",
    "df = pd.read_table('SMSSpamCollection', header = None, encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2821b6b1-06f1-4188-9782-50fbc5fe129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "#useful information about the data set: ~5000 data and have two columns, 0: spam/ham, 1:actual msg body\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75a3b7c-588e-4853-88c8-86ea3e7579ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check class distribution\n",
    "classes = df[0]\n",
    "print(classes.value_counts()) #prints unique values in first column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bbdf4-57ac-44ac-bff3-820944233c4d",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "Convert the class labels (span/ham) to binray values uesing sklearn LabelEncoder. \n",
    "Replace email address, url, phonenumbers with generic representation. \n",
    "Last, remove stop words and keepwing word stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f297ee-fcb9-4b4d-9e4c-0eb4549feb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels to binary values, 0 = ham, 1 = spam using sklearn label encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder() \n",
    "Y = encoder.fit_transform(classes) #classes is the df[0]. the first column of our data\n",
    "#print(classes[:10]) #printing first 10 to check if conversion is successful\n",
    "#print(Y[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218c1ac1-3498-43fa-b59c-42fdc2d1503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the SMS message data\n",
    "text_messages = df[1]\n",
    "# print(text_messages[:10]) #printing first 10 to check if text_msg is stored successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9150a7b0-0964-4693-9688-f59e65e2930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to replace strings with generic placeholders, to make algo understand better\n",
    "# e.g. emailaddresses with 'email'  replace urls with'url', 0945234 with 'phonenumbers'\n",
    "# https://regexlib.com/ can be used to search for already generated regex\n",
    "\n",
    "# Replace email addresses with 'emailaddress'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',' emailaddress ', regex=True)\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',' webaddress ', regex=True)\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'£|\\$', ' moneysymb ', regex=True)\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',' phonenumbr ', regex=True)\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', ' numbr ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c752cc-bee6-43ba-8ee5-987d5b305e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ', regex=True)\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2953cef8-151d-4856-bab6-60974187feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change words to lower case - Hello, HELLO, hello are all the same word\n",
    "processed = processed.str.lower()\n",
    "#print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e439dd3-6f73-4d53-a734-b0f0e5ea909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from text messages\n",
    "# stopwords = words generally filtered out before processing a natural langugage (e.g. for, the, ...)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "\n",
    "#loopthrough the set, x is the text msg, x.split() is the words in text msg, and join(append) all words that are NOT in stop_words\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90020d90-4e70-47d6-af38-7e719cbe8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only word stems using a Porter stemmer\n",
    "# e.g. removing the tenses & ings... \n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "#for all words(term) in x.split(), we stem it (ps.stem(term)) and join(append) all the stemmed term\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84f1e7f-2991-49ff-9111-0c8801f1bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazi avail bugi n great world...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri numbr wkli comp win fa cup final tk...\n",
      "3                     u dun say earli hor u c alreadi say\n",
      "4                    nah think goe usf live around though\n",
      "                              ...                        \n",
      "5567    numbr nd time tri numbr contact u u moneysymb ...\n",
      "5568                              ü b go esplanad fr home\n",
      "5569                                    piti mood suggest\n",
      "5570    guy bitch act like interest buy someth els nex...\n",
      "5571                                       rofl true name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed) #print out the processed content after text-cleaning, removing stoplist, removing wordstems...etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e1daa-a454-4990-b477-dbddfbf85bc3",
   "metadata": {},
   "source": [
    "## 3. Generating Features\n",
    "Features for machine learning algorithms in NLP are generally words in each text message. For this purpose, it will be necessary to tokenize each word and choose the top 500-1500 most common words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6759434b-4a7c-4c24-82b6-7c5e6ae093f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "\n",
    "#build a nltk frequency distribution list of the tokenized words\n",
    "all_words = nltk.FreqDist(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2abc6823-d8bd-43dd-87e6-60ded63e239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6318\n"
     ]
    }
   ],
   "source": [
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "#print('Most common words: {}'.format(all_words.most_common(500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc06d81e-6ae6-428e-8261-f678e7255eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "#the more features, the longer time needed for training, so just choose 500~1500 as an adequate amount\n",
    "word_features = all_words.most_common(1000)\n",
    "word_features = [item[0] for item in word_features]\n",
    "\n",
    "#print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07514820-8a67-4c6b-a059-67bee230b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# # Checking output of the first sentence\n",
    "# features = find_features(processed[0])\n",
    "# for key, value in features.items():\n",
    "#     if value == True:\n",
    "#         print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62031574-36dc-4852-90b8-dcc084429dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets find features for all the messages\n",
    "messages = list(zip(processed, Y))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages) #shuffle the text messages so that the spam and ham are naturally distributed\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "# after zip, the messages are in text, label(Y) format\n",
    "# the find_features(text) will return a list of most common words with true/false value depending on whether the common word are in that sentence\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "326dd718-bc74-4ea7-b76f-eb514dfb82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sets:  4179\n",
      "Testing data sets:  1393\n"
     ]
    }
   ],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)\n",
    "print(\"Training data sets: \",len(training)) #75% of original data set\n",
    "print(\"Testing data sets: \",len(testing)) #25% of original data set sinze we set test_size = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ec09a-5c14-401d-b6ac-124c397ffa74",
   "metadata": {},
   "source": [
    "## 4. Scikit-Learn Classifiers with NLTK\n",
    "Train different models and compare their performance. Import all needed algo from sklearn and import some performance metrics, such as accuracy_score and classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e138db7-79c3-4894-a30a-a19d722a9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "# we will use classifier from sklearn and wrapping them with nltk and deploying it in the notebook\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC #supportvectorclassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Below code trains one model only\n",
    "# model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# # train the model on the training data\n",
    "# model.train(training)\n",
    "\n",
    "# # and test on the testing dataset!\n",
    "# accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "# print(\"SVC Accuracy: {}\".format(accuracy))\n",
    "\n",
    "# Below code trains multiple models at the same time \n",
    "# Define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100), #need to set the max iteration for SGDClassifier model\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear') #set the kernal to run linear model\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52719cf-491b-47db-8793-16f2312edde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 95.5491744436468%\n",
      "Decision Tree Accuracy: 96.98492462311557%\n",
      "Random Forest Accuracy: 98.85139985642498%\n",
      "Logistic Regression Accuracy: 99.06676238334529%\n",
      "SGD Classifier Accuracy: 98.99497487437185%\n",
      "Naive Bayes Accuracy: 98.85139985642498%\n",
      "SVM Linear Accuracy: 98.92318736539842%\n",
      "--All models completed--\n",
      "Logistic Regression Model provides the best accuracy of: 99.06676238334529%\n"
     ]
    }
   ],
   "source": [
    "#run each model and show the accuracy of each model\n",
    "bestModelName = ''\n",
    "bestModelAccuracy = 0.01\n",
    "for name, classifiers in models:\n",
    "    nltk_model = SklearnClassifier(classifiers) #define the model\n",
    "    nltk_model.train(training) #train the model with the training dataset\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100 # measure the accuracy of the model by passing in the testing dataset\n",
    "    print(\"{} Accuracy: {}%\".format(name, accuracy))\n",
    "    if accuracy>bestModelAccuracy:\n",
    "        bestModelAccuracy = accuracy\n",
    "        bestModelName = name\n",
    "\n",
    "print(\"--All models completed--\")\n",
    "print(\"{} Model provides the best accuracy of: {}%\".format(bestModelName, bestModelAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5d9d7ef-5be4-4cbe-bf92-c6dcc0400ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 99.21033740129216\n",
      "Ensemble method improve overall accuracy by 0.14357501794687266 %\n"
     ]
    }
   ],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "# Have all the models to vote on one text msg of its classification\n",
    "#so instead of relying on only one algorithm, we can rely on all, and take the result from the most models\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "#hard voting is a binary output of each, soft voting will output class probability\n",
    "#n_jobs = -1,  means all cores are used in training the model in parallel  if set to 2 then will only use 2 cores\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = list(models), voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))\n",
    "if accuracy>bestModelAccuracy:\n",
    "    print(\"Ensemble method improve overall accuracy by\", accuracy - bestModelAccuracy, \"%\")\n",
    "else:\n",
    "    print(\"Ensemble method does not improve overall accuracy. Perhaps because all models failing at similar edge cases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2deb55a-546c-4ab8-b235-1855e5a51f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1e15d23-4b3c-4f25-b508-a74b8802df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1200\n",
      "           1       0.99      0.95      0.97       193\n",
      "\n",
      "    accuracy                           0.99      1393\n",
      "   macro avg       0.99      0.97      0.98      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>10</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1199    1\n",
       "       spam        10  183"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
